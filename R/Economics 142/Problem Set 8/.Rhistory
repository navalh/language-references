which(x == 4) == which(x == 4)[[1]]
x <- c(1,2,3,4,1,2,3,4)
which(x == 1)
#Question 1a)
ps8 <- read.csv("C:/Users/Naval/Dropbox/Berkeley Documents/Fall 2016/Econ C142/Problem Set 8/ps8.csv", header = TRUE, sep = ",")
ps8$two_fos <- floor((ps8$FOD1P)/100)
set.seed(5)
train = sample(1:nrow(ps8), nrow(ps8)/2)
test = (-train)
ps8train = ps8[train,]; ps8test = ps8[test,]
#Question 1b)
v <- sort(unique(ps8train$two_fos))
mean_wage_v <- rep(0, length(v))
count_v <- rep(0, length(v))
for (i in 1:length(v)){
mean_wage_v[i] = mean(subset(ps8train,two_fos == v[i])$WAGP)
count_v[i] = length(subset(ps8train,two_fos == v[i])$WAGP)
}
grandmean_wage = mean(ps8train$WAGP)
N = length(ps8train$WAGP)
#Question 1c)
theta_v = rep(0, length(v))
mean_wage_hat_v = rep(0, length(v))
msfe_1 = rep(0, 100)
for (k in 1:100) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8train$WAGP[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
msfe_1
msfe_1 = rep(0, 10^6)
for (k in 1:10^6) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8train$WAGP[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
msfe_1
msfe_1[100]
msfe_1[200]
msfe_1[150]
theta_v = rep(0, length(v))
mean_wage_hat_v = rep(0, length(v))
msfe_1 = rep(0, 50)
for (k in 200:250) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8train$WAGP[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
msfe_1
#Question 1c)
theta_v = rep(0, length(v))
mean_wage_hat_v = rep(0, length(v))
msfe_1 = rep(0, 50)
for (k in 201:251) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k-199] = msfe_1[k-199] + (ps8train$WAGP[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k-199] = msfe_1[k-199]/N
}
msfe_1
theta_v = rep(0, length(v))
mean_wage_hat_v = rep(0, length(v))
msfe_1 = rep(0, 100)
#Using 100 values for potential k: 10, 20,... 1000
for (k in 1:100) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*10*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8train$WAGP[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
msfe_1
mean_wage_v
mean_wage_hat_v
#Question 1a)
ps8 <- read.csv("C:/Users/Naval/Dropbox/Berkeley Documents/Fall 2016/Econ C142/Problem Set 8/ps8.csv", header = TRUE, sep = ",")
ps8$two_fos <- floor((ps8$FOD1P)/100)
ps8$logwage <- log(ps8$WAGP)
set.seed(5)
train = sample(1:nrow(ps8), nrow(ps8)/2)
test = (-train)
ps8train = ps8[train,]; ps8test = ps8[test,]
#Question 1b)
v <- sort(unique(ps8train$two_fos))
mean_wage_v <- rep(0, length(v))
count_v <- rep(0, length(v))
for (i in 1:length(v)){
mean_wage_v[i] = mean(subset(ps8train,two_fos == v[i])$logwage)
count_v[i] = length(subset(ps8train,two_fos == v[i])$logwage)
}
grandmean_wage = mean(ps8train$logwage)
N = length(ps8train$logwage)
#Question 1c)
theta_v = rep(0, length(v))
mean_wage_hat_v = rep(0, length(v))
msfe_1 = rep(0, 100)
#Using 100 values for potential k: 10, 20,... 1000
for (k in 1:100) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*10*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8train$logwage[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
msfe_1
msfe_1 = rep(0, 10)
#Using 100 values for potential k: 10, 20,... 1000
for (k in 1:10) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8train$logwage[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
msfe_1
msfe_1 = rep(0, 10)
#Using 100 values for potential k: 10, 20,... 1000
for (k in 1:10) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*1000*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8train$logwage[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
msfe_1
for (k in 1:10) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*100000*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8train$logwage[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
msfe_1
for (k in 1:100) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*10*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8train$logwage[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
msfe_1
msfe_1 = rep(0, 100)
#Using 100 values for potential k: 10, 20,... 1000
for (k in 1:100) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*10*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8train$logwage[i] - mean_wage_hat_v[which(v == ps8train$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
msfe_1
library(ggplot2)
ggplot(msfe_1, aes(x=1:100, y=msfe_1[x])) + geom_point(shape=1)
ggplot(msfe_1, aes(x=1:100, y=msfe_1)) + geom_point(shape=1)
plot(x = seq(1,1000, 10), y = msfe_1, xlab = "k", ylab = "MSFE", main = "MSFE varying with k, under Shrinkage Regression")
#Question 1c)
N = length(ps8test$logwage)
theta_v = rep(0, length(v))
mean_wage_hat_v = rep(0, length(v))
msfe_1 = rep(0, 100)
#Using 100 values for potential k: 10, 20,... 1000
for (k in 1:100) {
for (i in 1:length(v)){
theta_v[i] = (N - count_v[i])/(N - count_v[i] + k*10*count_v[i])
mean_wage_hat_v[i] = theta_v[i]*grandmean_wage + (1 - theta_v[i])*mean_wage_v[i]
}
for (i in 1:N) {
msfe_1[k] = msfe_1[k] + (ps8test$logwage[i] - mean_wage_hat_v[which(v == ps8test$two_fos[i])])^2
}
msfe_1[k] = msfe_1[k]/N
}
plot(x = seq(1,1000, 10), y = msfe_1, xlab = "k", ylab = "MSFE", main = "MSFE varying with k, under Shrinkage Regression")
#The MSFE continues to decrease with higher k, implying the optimal k is k = infinity (causing theta_v = mean_wage_v)
mean_wage_hat_v = rep(0, length(v))
library(ISLR)
install.packages(dummies)
install.packages("dummies"")
install.packages("dummies")
install.packages("dummies")
library(ISLR)
x = dummy(ps8train$two_fos)
y = ps8train$logwage
grid = 10^seq(-2, 10, length = 100)
ridge.mod = glmnet(x, y, alpha = 0, lambda = grid, standardize=FALSE )
library(glmnet)
library(dummies)
x = dummy(ps8train$two_fos)
y = ps8train$logwage
grid = 10^seq(-2, 10, length = 100)
ridge.mod = glmnet(x, y, alpha = 0, lambda = grid, standardize=FALSE )
install.packages("glmnet")
library(glmnet)
library(dummies)
x = dummy(ps8train$two_fos)
y = ps8train$logwage
grid = 10^seq(-2, 10, length = 100)
ridge.mod = glmnet(x, y, alpha = 0, lambda = grid, standardize=FALSE )
ridge.mod
grid = 10^seq(-2, 2, length = 100)
ridge.mod = glmnet(x, y, alpha = 0, lambda = grid, standardize=FALSE )
ridge.mod
grid = seq(1,100,1)
ridge.mod = glmnet(x, y, alpha = 0, lambda = grid, standardize=FALSE )
ridge.mod
library(glmnet)
library(dummies)
x_train = dummy(ps8train$two_fos); x_test = dummy(ps8test$two_fos)
y_train = ps8train$logwage; y_test = ps8test$logwage
grid = seq(1,100,1)
ridge.mod = glmnet(x_train, y_train, alpha = 0, lambda = grid, standardize=FALSE )
msfe_2 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge.pred=predict(ridge.mod, s=lmbda, newx=x_test)
msfe_2[lmbda] = mean((ridge.pred-y_test)^2)
}
plot(x = grid, y = msfe_2, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Ridge Regression")
#Th
grid
length(grid)
lmbda
ridge.pred=predict(ridge.mod, s=lmbda, newx=x_test)
library(glmnet)
library(dummies)
x_train = dummy(ps8train$two_fos); x_test = dummy(ps8test$two_fos)
y_train = ps8train$logwage; y_test = ps8test$logwage
grid = seq(1,100,1)
ridge.mod = glmnet(x_train, y_train, alpha = 0, lambda = grid, standardize=FALSE )
msfe_2 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge.pred=predict(ridge.mod, s=lmbda, newx =x_test)
msfe_2[lmbda] = mean((ridge.pred-y_test)^2)
}
plot(x = grid, y = msfe_2, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Ridge Regression")
#Th
ridge.mod$lambda
ridge.mod[1]
ridge.mod[2]
grid = seq(1,100,1)/10
ridge.mod = glmnet(x_train, y_train, alpha = 0, lambda = grid, standardize=FALSE )
msfe_2 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge.pred=predict(ridge.mod, s=lmbda/10, newx =x_test)
msfe_2[lmbda] = mean((ridge.pred-y_test)^2)
}
plot(x = grid, y = msfe_2, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Ridge Regression")
#Th
ridge.mod
msfe_2 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge_pred=predict(ridge.mod, s=lmbda/10, newx =x_test)
msfe_2[lmbda] = mean((ridge_pred-y_test)^2)
}
plot(x = grid, y = msfe_2, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Ridge Regression")
#Th
dim(x_train)
dim(x_test)
dim(y_test)
dim(y_train)
set.seed(2)
train = sample(1:nrow(ps8), nrow(ps8)/2)
test = (-train)
ps8train = ps8[train,]; ps8test = ps8[test,]
x_train = dummy(ps8train$two_fos); x_test = dummy(ps8test$two_fos)
dim(x_train)
dim(x_test)
x_test
head(x_test)
head(x_train)
set.seed(4)
train = sample(1:nrow(ps8), nrow(ps8)/2)
test = (-train)
ps8train = ps8[train,]; ps8test = ps8[test,]
x_train = dummy(ps8train$two_fos); x_test = dummy(ps8test$two_fos)
dim(x_train)
dim(x_test)
count_v
length(count_v)
for (i in 1:length(v)){
mean_wage_v[i] = mean(subset(ps8train,two_fos == v[i])$logwage)
count_v[i] = length(subset(ps8test,two_fos == v[i])$logwage)
}
length(count_v)
count_v
v[17]
set.seed(5)
train = sample(1:nrow(ps8), nrow(ps8)/2)
test = (-train)
ps8train = ps8[train,]; ps8test = ps8[test,]
which(ps8train$two_fos == 35)
which(ps8test$two_fos == 35)
train = sample(1:nrow(ps8), nrow(ps8)/2)
test = (-train)
ps8train = ps8[train,]; ps8test = ps8[test,]
print(which(ps8train$two_fos == 35))
print(which(ps8test$two_fos == 35))
which(ps8test$two_fos == 35)
which(ps8train$two_fos == 35)
train = sample(1:nrow(ps8), nrow(ps8)/2)
test = (-train)
ps8train = ps8[train,]; ps8test = ps8[test,]
print(which(ps8train$two_fos == 35))
print(which(ps8test$two_fos == 35))
set.seed(5)
train = sample(1:nrow(ps8), nrow(ps8)/2)
test = (-train)
ps8train = ps8[train,]; ps8test = ps8[test,]
print(which(ps8train$two_fos == 35))
print(which(ps8test$two_fos == 35))
set.seed(6)
train = sample(1:nrow(ps8), nrow(ps8)/2)
test = (-train)
ps8train = ps8[train,]; ps8test = ps8[test,]
print(which(ps8train$two_fos == 35))
print(which(ps8test$two_fos == 35))
set.seed(7)
train = sample(1:nrow(ps8), nrow(ps8)/2)
test = (-train)
ps8train = ps8[train,]; ps8test = ps8[test,]
print(which(ps8train$two_fos == 35))
print(which(ps8test$two_fos == 35))
library(glmnet)
library(dummies)
x_train = dummy(ps8train$two_fos); x_test = dummy(ps8test$two_fos)
y_train = ps8train$logwage; y_test = ps8test$logwage
grid = seq(1,100,1)
ridge.mod = glmnet(x_train, y_train, alpha = 0, lambda = grid, standardize=FALSE )
msfe_2 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge_pred=predict(ridge.mod, s=lmbda, newx =x_test)
msfe_2[lmbda] = mean((ridge_pred-y_test)^2)
}
plot(x = grid, y = msfe_2, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Ridge Regression")
#Th
grid = seq(1,100,1)/10
ridge.mod = glmnet(x_train, y_train, alpha = 0, lambda = grid, standardize=FALSE )
msfe_2 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge_pred=predict(ridge.mod, s=lmbda/10, newx =x_test)
msfe_2[lmbda] = mean((ridge_pred-y_test)^2)
}
plot(x = grid, y = msfe_2, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Ridge Regression")
#Th
ridge.mod_l = glmnet(x_train, y_train, alpha = 1, lambda = grid, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge_pred=predict(ridge.mod_l, s=lmbda/10, newx =x_test)
msfe_3[lmbda] = mean((ridge_pred-y_test)^2)
}
plot(x = grid, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
ridge.mod_l = glmnet(x_train, y_train, alpha = 1, lambda = grid*10, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge_pred=predict(ridge.mod_l, s=lmbda, newx =x_test)
msfe_3[lmbda] = mean((ridge_pred-y_test)^2)
}
plot(x = grid*10, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
#
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid*10, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
lasso_pred=predict(lasso.mod, s=lmbda, newx =x_test)
msfe_3[lmbda] = mean((lasso_pred-y_test)^2)
}
plot(x = grid*10, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
#
ridge.mod = glmnet(x_train, y_train, alpha = 0, lambda = grid/10, standardize=FALSE)
msfe_2 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge_pred=predict(ridge.mod, s=lmbda/10, newx =x_test)
msfe_2[lmbda] = mean((ridge_pred-y_test)^2)
}
plot(x = grid, y = msfe_2, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Ridge Regression")
#The MSFE continues to increase with higher lambda,
grid = seq(1,100,1)
ridge.mod = glmnet(x_train, y_train, alpha = 0, lambda = grid, standardize=FALSE)
msfe_2 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge_pred=predict(ridge.mod, s=lmbda, newx =x_test)
msfe_2[lmbda] = mean((ridge_pred-y_test)^2)
}
plot(x = grid, y = msfe_2, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Ridge Regression")
#The MSFE continues to increase with higher lambda, implying the optimal lambda is lambda = 0 (causing beta_v = mean_wage_v)
#The MSFE of the Ridge regression almost looks like the reciprocal of the MSFE for the Shrinkage regression.
grid = seq(1,10,0.1)
grid
grid = seq(1,100,1)/10
grid
y_train = ps8train$logwage; y_test = ps8test$logwage
#Using 100 values for potential lambda: 0.1, 0.2, ..., 10
grid = seq(1,100,1)/10
ridge.mod = glmnet(x_train, y_train, alpha = 0, lambda = grid, standardize=FALSE)
msfe_2 = rep(0,100)
for (lmbda in 1:length(grid)){
ridge_pred=predict(ridge.mod, s=lmbda/10, newx =x_test)
msfe_2[lmbda] = mean((ridge_pred-y_test)^2)
}
plot(x = grid, y = msfe_2, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Ridge Regression")
#The MSFE continues to increase with higher lambda, implying the optimal lambda is lambda = 0 (causing beta_v = mean_wage_v)
#The MSFE of the Ridge regression almost looks like the reciprocal of the MSFE for the Shrinkage regression.
#Question 3
grid_lasso = 10^seq(-2, 4, length = 100)
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid_lasso, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
lasso_pred = predict(lasso.mod, s= grid_lasso[lmbda], newx =x_test)
msfe_3[lmbda] = mean((lasso_pred-y_test)^2)
}
plot(x = grid_lasso, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
#
grid_lasso = 10^seq(-2, 2, length = 100)
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid_lasso, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
lasso_pred = predict(lasso.mod, s= grid_lasso[lmbda], newx =x_test)
msfe_3[lmbda] = mean((lasso_pred-y_test)^2)
}
plot(x = grid_lasso, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
#
grid_lasso = 10^seq(-2, 1, length = 100)
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid_lasso, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
lasso_pred = predict(lasso.mod, s= grid_lasso[lmbda], newx =x_test)
msfe_3[lmbda] = mean((lasso_pred-y_test)^2)
}
plot(x = grid_lasso, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
#
grid_lasso = 10^seq(-2, 0, length = 100)
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid_lasso, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
lasso_pred = predict(lasso.mod, s= grid_lasso[lmbda], newx =x_test)
msfe_3[lmbda] = mean((lasso_pred-y_test)^2)
}
plot(x = grid_lasso, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
#
grid_lasso = 10^seq(-2, -1, length = 100)
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid_lasso, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
lasso_pred = predict(lasso.mod, s= grid_lasso[lmbda], newx =x_test)
msfe_3[lmbda] = mean((lasso_pred-y_test)^2)
}
plot(x = grid_lasso, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
#
grid_lasso = 10^seq(-3, -0.5, length = 100)
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid_lasso, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
lasso_pred = predict(lasso.mod, s= grid_lasso[lmbda], newx =x_test)
msfe_3[lmbda] = mean((lasso_pred-y_test)^2)
}
plot(x = grid_lasso, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
#
grid_lasso = 10^seq(-3, -1.5, length = 100)
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid_lasso, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
lasso_pred = predict(lasso.mod, s= grid_lasso[lmbda], newx =x_test)
msfe_3[lmbda] = mean((lasso_pred-y_test)^2)
}
plot(x = grid_lasso, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
#
grid_lasso = 10^seq(-3, -1, length = 100)
lasso.mod = glmnet(x_train, y_train, alpha = 1, lambda = grid_lasso, standardize=FALSE)
msfe_3 = rep(0,100)
for (lmbda in 1:length(grid)){
lasso_pred = predict(lasso.mod, s= grid_lasso[lmbda], newx =x_test)
msfe_3[lmbda] = mean((lasso_pred-y_test)^2)
}
plot(x = grid_lasso, y = msfe_3, xlab = "lambda", ylab = "MSFE", main = "MSFE varying with k, under Lasso Regression")
#
